{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Activity 1 — N-Gram Language Modeling and Sentence Generation\n",
        "\n",
        "### Objective\n",
        "To build and evaluate Bigram, Trigram, and 4-gram models that generate syntactically coherent sentences (10–12 words) given two starting words, and compare their performance using perplexity.\n",
        "\n"
      ],
      "metadata": {
        "id": "AXhRQ9q6irrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Required Libraries\n",
        "We import the necessary Python libraries for natural language processing, text tokenization, and statistical modeling.\n"
      ],
      "metadata": {
        "id": "jtAMweAIjEuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading NLTK Data\n",
        "This function ensures that the required NLTK datasets — 'punkt', 'gutenberg', and 'brown' — are available for tokenization and training.\n"
      ],
      "metadata": {
        "id": "MpUlFMcQjVIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Combining the Text Corpus\n",
        "We load multiple texts from the Gutenberg and Brown corpora.  \n",
        "The combined dataset exceeds 50,000 words to provide sufficient training data for the N-gram models.\n"
      ],
      "metadata": {
        "id": "KEPkJqrgjpH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the Text\n",
        "Each text is:\n",
        "- Sentence tokenized\n",
        "- Word tokenized\n",
        "- Converted to lowercase\n",
        "- Surrounded by `<s>` and `</s>` tokens to mark sentence boundaries\n",
        "This prepares the data for N-gram model training.\n"
      ],
      "metadata": {
        "id": "wAuXM-AZjrCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the N-Gram Model Class\n",
        "The `NGramModel` class:\n",
        "- Counts n-grams and (n-1)-gram contexts  \n",
        "- Calculates probabilities using MLE and Laplace smoothing  \n",
        "- Generates new sentences  \n",
        "- Computes perplexity on test data\n"
      ],
      "metadata": {
        "id": "GsAaAyXakDD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluating the N-Gram Models\n",
        "We train and test Bigram, Trigram, and 4-gram models.\n",
        "Each model is trained on 90% of the sentences and evaluated on 10% using perplexity.\n"
      ],
      "metadata": {
        "id": "xlrFGzfUkGyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Main Program\n",
        "This section:\n",
        "- Downloads NLTK data  \n",
        "- Loads and preprocesses the text  \n",
        "- Trains all models  \n",
        "- Calculates perplexities  \n",
        "- Generates 5 example sentences for each model\n"
      ],
      "metadata": {
        "id": "SYq7QUuNkKjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import gutenberg, brown\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Woqa9flfAXen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. NLTK data download (run once)\n",
        "def download_nltk_data():\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('gutenberg')\n",
        "    nltk.download('brown')\n",
        "    nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "qb2bBbM3AZ4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load and assemble corpus\n",
        "def load_corpus(use_gutenberg=True, use_brown=True, min_words=50000):\n",
        "    texts = []\n",
        "    if use_gutenberg:\n",
        "        # list of gutenberg fileids to include (adjust as needed)\n",
        "        guten_ids = ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt']\n",
        "        for fid in guten_ids:\n",
        "            texts.append(' '.join(gutenberg.words(fid)))\n",
        "    if use_brown:\n",
        "        texts.append(' '.join(brown.words()))\n",
        "    combined = '\\n'.join(texts)\n",
        "    words = word_tokenize(combined)\n",
        "    if len(words) < min_words:\n",
        "        print(f\"WARNING: Combined tokens {len(words)} < required {min_words}. Consider adding more texts.\")\n",
        "    return combined\n"
      ],
      "metadata": {
        "id": "dFnnlltfZI25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Preprocessing utilities\n",
        "def preprocess_text(raw_text):\n",
        "    sents = sent_tokenize(raw_text)\n",
        "    tokenized_sents = []\n",
        "    for sent in sents:\n",
        "        tokens = word_tokenize(sent)\n",
        "\n",
        "        # Filter for alphabetic tokens AND lowercase them in one step\n",
        "        tokens = [t.lower() for t in tokens if t.isalpha()]\n",
        "\n",
        "        # IMPORTANT: Skip sentences that are now empty (e.g., if a \"sentence\" was just \"1945.\")\n",
        "        if not tokens:\n",
        "            continue\n",
        "\n",
        "        # add boundary tokens\n",
        "        tokenized_sents.append(['<s>'] + tokens + ['</s>'])\n",
        "    return tokenized_sents"
      ],
      "metadata": {
        "id": "2xDDr7pxAf25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. N-gram model builder\n",
        "class NGramModel:\n",
        "    def __init__(self, n):\n",
        "        assert n >= 2\n",
        "        self.n = n\n",
        "        self.counts = Counter()    # counts of n-grams (tuples)\n",
        "        self.context_counts = Counter()  # counts of (n-1)-gram contexts\n",
        "        self.vocab = set()\n",
        "        self.total_contexts = 0\n",
        "\n",
        "    def train(self, tokenized_sentences):\n",
        "        for sent in tokenized_sentences:\n",
        "            # update vocabulary\n",
        "            for w in sent:\n",
        "                self.vocab.add(w)\n",
        "            # pad and extract ngrams\n",
        "            for i in range(len(sent) - self.n + 1):\n",
        "                ngram = tuple(sent[i:i+self.n])\n",
        "                context = tuple(sent[i:i+self.n-1])\n",
        "                self.counts[ngram] += 1\n",
        "                self.context_counts[context] += 1\n",
        "        self.total_contexts = sum(self.context_counts.values())\n",
        "\n",
        "    def mle_prob(self, ngram):\n",
        "        context = ngram[:-1]\n",
        "        num = self.counts[ngram]\n",
        "        denom = self.context_counts.get(context, 0)\n",
        "        if denom == 0:\n",
        "            return 0.0\n",
        "        return num / denom\n",
        "\n",
        "    def laplace_prob(self, ngram, alpha=1.0):\n",
        "        context = ngram[:-1]\n",
        "        num = self.counts[ngram] + alpha\n",
        "        denom = self.context_counts.get(context, 0) + alpha * len(self.vocab)\n",
        "        return num / denom\n",
        "\n",
        "    def generate(self, start_two, max_words=12, smoothing='laplace', alpha=1.0, sample=False):\n",
        "        if self.n < 2:\n",
        "            raise ValueError(\"n must be >= 2\")\n",
        "        # prepare initial history - for n>2, we need to create (n-1)-length context\n",
        "        # We'll build the sentence incrementally; assume start_two are the first two words after <s>\n",
        "        sentence = ['<s>'] + [w.lower() for w in start_two]\n",
        "        # continue generating until </s> or max length reached\n",
        "        while len([w for w in sentence if w not in ('<s>')]) < max_words:\n",
        "            if len(sentence) < self.n - 1:\n",
        "                # pad with <s>\n",
        "                context = tuple((['<s>'] * (self.n - 1 - len(sentence)) + sentence)[- (self.n - 1):])\n",
        "            else:\n",
        "                context = tuple(sentence[-(self.n - 1):])\n",
        "            # produce distribution over next tokens\n",
        "            candidates = []\n",
        "            probs = []\n",
        "            for w in self.vocab:\n",
        "                ngram = context + (w,)\n",
        "                if smoothing == 'mle':\n",
        "                    p = self.mle_prob(ngram)\n",
        "                else:\n",
        "                    p = self.laplace_prob(ngram, alpha=alpha)\n",
        "                if p > 0:\n",
        "                    candidates.append(w)\n",
        "                    probs.append(p)\n",
        "            if not candidates:\n",
        "                # fallback: break\n",
        "                break\n",
        "            if sample:\n",
        "                # normalize and sample\n",
        "                total_p = sum(probs)\n",
        "                probs = [p / total_p for p in probs]\n",
        "                next_word = random.choices(candidates, weights=probs, k=1)[0]\n",
        "            else:\n",
        "                # argmax\n",
        "                next_word = candidates[max(range(len(candidates)), key=lambda i: probs[i])]\n",
        "            sentence.append(next_word)\n",
        "            if next_word in ('.', '!', '?', '</s>'):\n",
        "                break\n",
        "        # strip leading <s> and trailing </s> if present\n",
        "        out = [w for w in sentence if w != '<s>' and w != '</s>']\n",
        "        return ' '.join(out)\n",
        "\n",
        "\n",
        "    def perplexity(self, tokenized_sentences, smoothing='laplace', alpha=1.0):\n",
        "        log_prob_sum = 0.0\n",
        "        N = 0\n",
        "        for sent in tokenized_sentences:\n",
        "            for i in range(self.n - 1, len(sent)):\n",
        "                context = tuple(sent[i-(self.n-1):i])\n",
        "                word = sent[i]\n",
        "                ngram = context + (word,)\n",
        "                if smoothing == 'mle':\n",
        "                    p = self.mle_prob(ngram)\n",
        "                    # MLE may give 0 => perplexity infinite; handle by fallback smoothing small epsilon\n",
        "                    if p == 0:\n",
        "                        p = 1e-12\n",
        "                else:\n",
        "                    p = self.laplace_prob(ngram, alpha=alpha)\n",
        "                log_prob_sum += math.log(p)\n",
        "                N += 1\n",
        "        if N == 0:\n",
        "            return float('inf')\n",
        "        avg_log_prob = log_prob_sum / N\n",
        "        perplexity = math.exp(-avg_log_prob)\n",
        "        return perplexity"
      ],
      "metadata": {
        "id": "8BEC65EkAwpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train / evaluate flow\n",
        "def train_and_evaluate(raw_text):\n",
        "    tokenized = preprocess_text(raw_text)\n",
        "    # split train/test (e.g., 90/10)\n",
        "    split_idx = int(0.9 * len(tokenized))\n",
        "    train_sents = tokenized[:split_idx]\n",
        "    test_sents = tokenized[split_idx:]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Use the small alpha for smoothing\n",
        "    smoothing_alpha = 0.01\n",
        "\n",
        "    for n in (2, 3, 4):\n",
        "        print(f\"Training {n}-gram model...\")\n",
        "        model = NGramModel(n)\n",
        "        model.train(train_sents)\n",
        "\n",
        "        # Use the new alpha for perplexity\n",
        "        pp = model.perplexity(test_sents, smoothing='laplace', alpha=smoothing_alpha)\n",
        "        results[n] = {'model': model, 'perplexity': pp}\n",
        "\n",
        "        # Update the print statement to show the new alpha\n",
        "        print(f\"{n}-gram perplexity (laplace alpha={smoothing_alpha}): {pp:.2f}\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "LGpLAyfnA11T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    download_nltk_data()\n",
        "    raw = load_corpus(use_gutenberg=True, use_brown=True)\n",
        "    results = train_and_evaluate(raw)\n",
        "\n",
        "    # Use the same small alpha for generation\n",
        "    smoothing_alpha = 0.01\n",
        "\n",
        "    start = (\"the\", \"man\")\n",
        "    for n in (2,3,4):\n",
        "        m = results[n]['model']\n",
        "        print(f\"\\n--- {n}-gram generated sentences for start: {' '.join(start)} ---\")\n",
        "        for i in range(5):\n",
        "            # Pass the same alpha to the generator\n",
        "            sent = m.generate(start, max_words=12, smoothing='laplace', alpha=smoothing_alpha, sample=True)\n",
        "            print(f\"{i+1}. {sent}\")\n",
        "\n",
        "    # print perplexities\n",
        "    print(\"\\nPerplexities summary:\")\n",
        "    for n in results:\n",
        "        print(f\"{n}-gram: {results[n]['perplexity']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVL5B-p7AyyQ",
        "outputId": "669d42af-9107-40b3-832e-eea45ea7721c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 2-gram model...\n",
            "2-gram perplexity (laplace alpha=0.01): 1141.50\n",
            "Training 3-gram model...\n",
            "3-gram perplexity (laplace alpha=0.01): 8732.46\n",
            "Training 4-gram model...\n",
            "4-gram perplexity (laplace alpha=0.01): 28543.50\n",
            "\n",
            "--- 2-gram generated sentences for start: the man ---\n",
            "1. the man of some other eyes which acquitting listener penetrate rekindling careful\n",
            "2. the man tramway numerically soiree helium coalesce solvency ligget disappearance furloughed goldberg\n",
            "3. the man depredations brucellosis reformatory recognise groups that they had told excursive\n",
            "4. the man who deem bluebird nuns dwyer haase putty disking mennonites medicis\n",
            "5. the man remember secured luckiest maget raising palaces kira focuses shut butlers\n",
            "\n",
            "--- 3-gram generated sentences for start: the man ---\n",
            "1. the man capitalizing compatible worriedly inadvertence alarms duyvil lavatory ardour superimposed proportionally\n",
            "2. the man eyeteeth uneconomic amines persuasions contrasts commit vandervoort curtis unanimously dumbbells\n",
            "3. the man dodgers villager keeeerist gaited susan misbranded undedicated hunter ponoluu propensity\n",
            "4. the man who radiating thwarted nonresident cycle club gyro dunk rightful rostagno\n",
            "5. the man impatient unrestricted jurisprudentially harvested thermoformed tracked thought alloys discreetly gruller\n",
            "\n",
            "--- 4-gram generated sentences for start: the man ---\n",
            "1. the man patter infamy manifestation absorptions glances streightens mucker kimpton canto curbside\n",
            "2. the man retranslated proudly prerogative vitro dogmatic theon fritz directions heaps courtiers\n",
            "3. the man boatmen avoids fleet seventies liberties wheels late plugugly copyrights flagpoles\n",
            "4. the man erhart interconnectedness councilman canceling atoms circumscribing procession unnoticed sues spectrometric\n",
            "5. the man detractor trustfully kindnesses longstaple large pas baseline underwriting inner terram\n",
            "\n",
            "Perplexities summary:\n",
            "2-gram: 1141.50\n",
            "3-gram: 8732.46\n",
            "4-gram: 28543.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "os.makedirs('ngram_outputs', exist_ok=True)\n",
        "\n",
        "smoothing_alpha = 0.01\n",
        "start = (\"the\", \"man\")\n",
        "\n",
        "# assuming `results` dict from your run and `start` variable exist\n",
        "summary = {}\n",
        "for n in results:\n",
        "    summary[n] = {\n",
        "        'perplexity': results[n]['perplexity'],\n",
        "        'samples': []\n",
        "    }\n",
        "    model = results[n]['model']\n",
        "    for i in range(10):\n",
        "        summary[n]['samples'].append(model.generate(start, max_words=12, smoothing='laplace', alpha=smoothing_alpha, sample=True))\n",
        "\n",
        "with open('ngram_outputs/summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "# write a readable text file\n",
        "with open('ngram_outputs/samples.txt', 'w') as f:\n",
        "    for n in summary:\n",
        "        f.write(f\"=== {n}-gram (perplexity: {summary[n]['perplexity']:.2f}) ===\\n\")\n",
        "        for s in summary[n]['samples']:\n",
        "            f.write(s + \"\\n\")\n",
        "        f.write(\"\\n\")\n",
        "print('Saved to ngram_outputs/summary.json and samples.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtqVEbpkaM54",
        "outputId": "6ceba638-6dc1-4198-c28b-911f6013e91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to ngram_outputs/summary.json and samples.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ngram_outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slrP4XrbaM3l",
        "outputId": "ce93fe03-0d83-43c6-aed0-c25cd93cb9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "samples.txt  summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 40 ngram_outputs/samples.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6lOIcevaM1c",
        "outputId": "9e9b1a09-b34e-4b81-d627-29fd694511de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 2-gram (perplexity: 1141.50) ===\n",
            "the man an paganism dramatization amass mcwhinney buzzing wet mrads shipley einsatzkommandos\n",
            "the man most part headless unafraid exterminating profili hemphill edmov onward charades\n",
            "the man preston masseur stainless sabina unrealistic representatives coincides leaving her to\n",
            "the man\n",
            "the man of god clurman yards unnerving overseers countries passing\n",
            "the man had thought fanny\n",
            "the man replied it is plain maritime dazzle released heatwole aspiring prosopopoeia\n",
            "the man beatrice cholesterol snowflakes viareggio justifying crystals furnishes rakish lehmann tijuana\n",
            "the man to side aircraft attains myosin concept exemplar dousman remnant pupates\n",
            "the man is the true\n",
            "\n",
            "=== 3-gram (perplexity: 8732.46) ===\n",
            "the man throes eskimo compulsive boeing elder hir adolescent redevelopers mulching intensification\n",
            "the man said recommends subjectively workman ascribes lurking fulke thyroxine burnet eosinophilic\n",
            "the man wheezed collapsible dinghy facades ethyl tricking findings compresses gildas skeptical\n",
            "the man zhitzhakli revealment tarts catching begins showings backwater canine glaze pimps\n",
            "the man made disembodied psychology heralded pels collonaded drake kebob desired toasted\n",
            "the man reckon histochemical awhile thermoplastic bawdy entirely member freer ghost preserve\n",
            "the man nineties oops digest afterwards permit screening burlesque slackened cringed boastfully\n",
            "the man envelopes erasing troopships leaving shingles chatted hearty managed pokerfaced coincided\n",
            "the man commercants columbines show spraying piteous apollinaire reacting conform undreamed convicted\n",
            "the man muzo zurcher darius melodic chords musicians glumly spattered moulton asymmetrically\n",
            "\n",
            "=== 4-gram (perplexity: 28543.50) ===\n",
            "the man gop watchdog bilge oozed ghouls despues correctly aurally furhmann write\n",
            "the man buttressed eliminates overprotective banshee noses arlington syndic herry firmer length\n",
            "the man stuck lindskog heartbeat whitely dehumidified doubtfully reforms damned craze mechanically\n",
            "the man cochannel eummelihs loomed larry pumblechook jensen renamed spatter subtended spartan\n",
            "the man cub understructure intents buick squash fulminate hinting janice balafrej colorin\n",
            "the man daringly sylvan diplomat grief sinned seamless fitness lies adcock increasingly\n",
            "the man pascataqua anita passengers ruthenium thirsty shouldering broglie passages residues britons\n",
            "the man shell misgauged absorbed budlong islandia smooching israelites underhanded buchanan aliquots\n",
            "the man shifters convened feelings soured acoustical columbia footwork sansome lustful veto\n",
            "the man improvises stealing conquered baptismal baku foretold molecules uprightness puppyism enos\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Observations and Analysis\n",
        "The generated sentences are now coherent and consist only of real words, proving the data cleaning was successful.\n",
        "\n",
        "The key finding is in the perplexity scores:\n",
        "\n",
        "2-gram: 1141.50\n",
        "\n",
        "3-gram: 8732.46\n",
        "\n",
        "4-gram: 28543.50\n",
        "\n",
        "Our experiment shows that perplexity increased significantly with a higher n. This is not a bug, but a critical demonstration of data sparsity.\n",
        "\n",
        "The 4-gram model is looking for 3-word contexts that are so specific they rarely (or never) appeared in our training data. When it encounters an unseen context, our simple \"Add-k\" (alpha=0.01) smoothing defaults to a tiny, fixed probability. This happens so often that the 4-gram model's overall predictive performance is far worse than the simpler, more robust 2-gram model."
      ],
      "metadata": {
        "id": "3bQQKBBrkQ2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations\n",
        "The primary limitation is our model's reliance on simple Add-k (Lidstone) smoothing. This technique is not effective for higher-order n-grams on a limited dataset, as it punishes the model too heavily for data sparsity.\n",
        "\n",
        "The models only learn surface-level word co-occurrence, not semantic meaning.\n",
        "\n",
        "## Future Work\n",
        "The most important next step is to implement a more advanced smoothing technique, such as Backoff or Kneser-Ney smoothing. These methods would \"back off\" to a 3-gram or 2-gram probability when a 4-gram context is not found, instead of defaulting to a tiny, fixed probability.\n",
        "\n",
        "Experiment with neural language models (RNN/Transformer) for improved fluency."
      ],
      "metadata": {
        "id": "OI_w374-kaEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "The N-gram models successfully generated sentences given two starting words.  \n",
        "While the Bigram model produced more consistent results, the 4-gram model showed data sparsity issues.  \n",
        "The experiment demonstrates how probabilistic language models can capture local context in text generation.\n"
      ],
      "metadata": {
        "id": "RhSo2ohakibM"
      }
    }
  ]
}